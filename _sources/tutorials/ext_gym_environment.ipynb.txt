{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9097e1f3",
   "metadata": {},
   "source": [
    "# Extension - Gym Environment Interface: minimal example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101b6db-e9c2-4f29-bc50-de5d3231f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotic as ry\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "print('ry version:', ry.__version__, ry.compiled())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85e735",
   "metadata": {},
   "source": [
    "Let's first create a configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ry.Config()\n",
    "C.addFile(ry.raiPath('scenarios/pandaSingle.g'))\n",
    "C.setJointState([.007], ['l_panda_finger_joint1']) #only cosmetics\n",
    "#the following makes it a LOT simpler\n",
    "C.setJointState([.0], ['l_panda_joint2'])\n",
    "C.setJointState([.7], ['l_panda_joint7'])\n",
    "gripper = 'l_gripper'\n",
    "\n",
    "box = C.addFrame('box') \\\n",
    "    .setShape(ry.ST.ssBox, size=[.1,.1,.1,.005]) .setColor([1,.5,0]) \\\n",
    "    .setPosition([.06,.35,.7]) \\\n",
    "    .setMass(.1) \\\n",
    "    .setAttributes({'friction': .5})\n",
    "box.setPosition([.1,.35,.7]) # this is the initial position of the box - change manually to make harder\n",
    "\n",
    "C.addFrame('plate', gripper) \\\n",
    "    .setShape(ry.ST.ssBox, size=[.02,.2,.36,.005]) .setColor([.5,1,0]) \\\n",
    "    .setRelativePosition([0,0,-.16]) \\\n",
    "    .setAttributes({'friction': .1})\n",
    "\n",
    "C.addFrame('target') \\\n",
    "    .setShape(ry.ST.marker, size=[.1]) .setColor([0,1,0]) \\\n",
    "    .setPosition([.3,.3,.7])\n",
    "\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35781d25",
   "metadata": {},
   "source": [
    "Here is a basic class implementing a gym.Env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e95379-5ff6-4b05-abaf-f356550653b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoboticGym(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    tau_env = .05 #a gym environment step corresponds to 0.05 sec\n",
    "    tau_sim = .01 #the underlying physics simulation is stepped with 0.01 sec (5 sim steps in each env step)\n",
    "    time = 0.\n",
    "    render_mode = 'human'\n",
    "\n",
    "    viewSteps = False\n",
    "    random_reset = False\n",
    "\n",
    "    def __init__(self, C:ry.Config, time_limit, sim_verbose=0):\n",
    "        self.time_limit = time_limit\n",
    "\n",
    "        # create/load the robotic configuration\n",
    "        self.C = C\n",
    "        self.q0 = self.C.getJointState()\n",
    "        self.X0 = self.C.getFrameState()\n",
    "        self.box = self.C.getFrame('box')\n",
    "        n = self.q0.size\n",
    "\n",
    "        # define the observation space (see also observation_fct())\n",
    "        self.observation_space = gym.spaces.Box(-2., +2., shape=(2*n + 7 + 6,), dtype=np.float32)\n",
    "\n",
    "        # define the action space\n",
    "        action_scale = 5\n",
    "        action_dim = n\n",
    "        self.action_space = gym.spaces.Box(-action_scale, +action_scale, shape=(action_dim,), dtype=np.float32)\n",
    "\n",
    "        # create the underlying physics simulation (using PhysX from Nvidia)\n",
    "        self.sim = ry.Simulation(self.C, ry.SimulationEngine.physx, verbose=sim_verbose)\n",
    "        self.state0 = self.sim.getState()\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.sim\n",
    "        del self.C\n",
    "\n",
    "    def step(self, action):\n",
    "        '''essential to implement gym.Env'''\n",
    "        self.step_qDelta(action)\n",
    "\n",
    "        self.time += self.tau_env\n",
    "        \n",
    "        observation = self.observation_fct()\n",
    "        reward = self.reward_fct()\n",
    "        terminated = False\n",
    "        truncated = (self.time >= self.time_limit) # terminated and truncated difference is super important\n",
    "        info = {\"no\": \"additional info\"}\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def observation_fct(self):\n",
    "        '''observations are joint pos/vel, and box pos/vel'''\n",
    "        state = self.sim.getState()\n",
    "        q = state.q\n",
    "        qDot = state.qDot\n",
    "        box_pos = state.freePos[0,:]\n",
    "        box_vel = state.freeVel[0,:,:].reshape(-1)\n",
    "        observation = np.concatenate((q, qDot, box_pos, box_vel), axis=0)\n",
    "        return observation\n",
    "\n",
    "    def reward_fct(self):\n",
    "        '''two reward terms: box-at-goal reward (peaked) and touch object reward (negDistance)'''\n",
    "        goalDist, _ = self.C.eval(ry.FS.positionDiff, [\"box\", \"target\"])\n",
    "        sigma = .2\n",
    "        r = 1. - np.tanh(np.linalg.norm(goalDist)/sigma)\n",
    "\n",
    "        negObjDistance, _ = self.C.eval(ry.FS.negDistance, [\"plate\", \"box\"])\n",
    "        r += negObjDistance[0]\n",
    "        return r\n",
    "\n",
    "    def step_qDelta(self, delta):\n",
    "        '''sending deltas in joint space; where deltas have semantic of velocity'''\n",
    "        # the sim needs to run at least with 100Hz (to properly simulate collisions)\n",
    "        # that's why we have multiple sim steps for each env step\n",
    "        sim_steps = int(self.tau_env/self.tau_sim)\n",
    "        delta[-1] *= 5.\n",
    "        for s in range(sim_steps):\n",
    "            #the following converts the deltas to position references for the underlying sim PD controller\n",
    "            q = self.sim.get_q()\n",
    "            self.sim.step(q + self.tau_sim * delta, self.tau_sim, ry.ControlMode.position)\n",
    "            if self.viewSteps:\n",
    "                self.C.view()\n",
    "                time.sleep(self.tau_sim)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        '''essential to implement gym.Env'''\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # X = self.state0.copy()\n",
    "        # if self.random_reset:\n",
    "        #     # resetting the box position to a random initial position -- makes it MUCH harder\n",
    "        #     X[self.box.ID, :2] += .3 * np.random.randn(2)\n",
    "\n",
    "        # resetting the sim\n",
    "        self.time = 0.\n",
    "        self.sim.resetTime()\n",
    "        self.sim.setState(self.state0)\n",
    "        self.sim.resetSplineRef()\n",
    "\n",
    "        observation = self.observation_fct()\n",
    "        info = {\"no\": \"additional info\"}\n",
    "        return observation, info\n",
    "        \n",
    "    def rollout(self, pi):\n",
    "        '''helper to play and view a policy'''\n",
    "        self.viewSteps=True #triggers display within the low level step functions\n",
    "        obs, info = self.reset()\n",
    "        t = 0\n",
    "        R = 0\n",
    "        while True:\n",
    "            action = pi(obs, t)\n",
    "            obs, reward, terminated, truncated, info = self.step(action)\n",
    "            R += reward\n",
    "            t += 1\n",
    "            print(\"reward: \", reward)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        print('total return:', R)\n",
    "        self.viewSteps=False #disables display within the low level step functions\n",
    "\n",
    "    def render(self):\n",
    "        '''also part of the env.Gym'''\n",
    "        self.C.view(False, f'RoboticGym time {self.time} / {self.time_limit}')\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self.C.view_getRgb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa708a",
   "metadata": {},
   "source": [
    "Note in particular the reward_fct and observation_fct above, using the `eval` methods to access geometric features.\n",
    "\n",
    "Let's first rollout a trivial policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f2904-b563-440a-8be8-20226544afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RoboticGym(C, time_limit=3., sim_verbose=0)  #verbosity of underlying sim - set to 2 once to see actual exploration during training\n",
    "\n",
    "pi = lambda obs, t : np.array([-1.,0.,-1.,0,0,0,0])\n",
    "env.rollout(pi)\n",
    "env.C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe485581",
   "metadata": {},
   "source": [
    "And we can train standard RL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99cf13a-44d5-4c5b-8e75-f0b5e3868f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./checkpoints_endeffectorActions/', name_prefix='rl_model')\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, gamma=0.99, learning_rate=3e-3, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "# model = PPO(\"MlpPolicy\", env, gamma=0.99, learning_rate=3e-3, n_steps=1024, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "model.learn(total_timesteps=1_000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c1e83",
   "metadata": {},
   "source": [
    "... and roll out the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c942846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SAC.load('./checkpoints_endeffectorActions/rl_model_199498_steps.zip', env=env)\n",
    "\n",
    "while True:\n",
    "    pi = lambda obs, t : model.predict(obs, deterministic=True)[0] #change deterministic to False to see with noise\n",
    "    env.rollout(pi)\n",
    "    if env.C.view(True, 'press q to stop looping') == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be9b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
