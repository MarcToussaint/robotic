{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9097e1f3",
   "metadata": {},
   "source": [
    "# Extension - Gym Environment Interface: minimal example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101b6db-e9c2-4f29-bc50-de5d3231f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotic as ry\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "print('ry version:', ry.__version__, ry.compiled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ry.Config()\n",
    "C.addFile(ry.raiPath('scenarios/pandaSingle.g'))\n",
    "C.view(False)\n",
    "\n",
    "C.addFrame('box') \\\n",
    "    .setShape(ry.ST.ssBox, size=[.1,.1,.1,.005]) .setColor([1,.5,0]) \\\n",
    "    .setPosition([.06,.35,.7]) \\\n",
    "    .setMass(.1) \\\n",
    "    .setAttribute('friction', 0.01) # friction definition # initial noise with falling box?\n",
    "\n",
    "C.addFrame('plate', 'l_gripper') \\\n",
    "    .setShape(ry.ST.ssBox, size=[.01,.2,.36,.0025]) .setColor([.5,1,0]) \\\n",
    "    .setRelativePosition([0,0,-.16])\n",
    "\n",
    "C.addFrame('target') \\\n",
    "    .setShape(ry.ST.marker, size=[.1]) .setColor([0,1,0]) \\\n",
    "    .setPosition([.4,.3,.7]) \\\n",
    "\n",
    "#the following would make it a LOT simpler\n",
    "C.setJointState([.0], ['l_panda_joint2'])\n",
    "C.setJointState([.7], ['l_panda_joint7'])\n",
    "\n",
    "C.setJointState([.007], ['l_panda_finger_joint1']) #only cosmetics\n",
    "\n",
    "q0 = C.getJointState()\n",
    "X0 = C.getFrameState()\n",
    "\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35781d25",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "\n",
    "The following code consists of a minimal Implementation of an environment in which the robot can operate. Currently the policy receives information about the internal state but no further information about the environment state. This is called open-loop Reinforcement Learning.\n",
    "\n",
    "What do you need to change to make it a closed-loop Reinforcement Learning setting? Apply the changes to the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e95379-5ff6-4b05-abaf-f356550653b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaiGym(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    tau = .05\n",
    "    time = 0.\n",
    "\n",
    "    def __init__(self, C, time_limit, reward_fct, render_mode=None):\n",
    "        self.C : ry.Config = C\n",
    "        self.time_limit = time_limit\n",
    "        self.reward_fct = reward_fct\n",
    "        self.render_mode = render_mode\n",
    "        # self.limits = self.C.getJointLimits()\n",
    "        self.limits = [-10., 10.]\n",
    "        self.q0 = self.C.getJointState()\n",
    "        self.X0 = self.C.getFrameState()\n",
    "\n",
    "        # here they need to configure open loop or closed loop RL, with and without box position\n",
    "        #self.observation_space = gym.spaces.box.Box(self.limits[0], self.limits[1], shape=(self.q0.size,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.box.Box(self.limits[0], self.limits[1], shape=(self.q0.size + 3,), dtype=np.float32)\n",
    "        \n",
    "        #self.action_space = gym.spaces.box.Box(low=-1., high=1., shape=(self.q0.size,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(low=np.array([-1, -1, -1]), high=np.array([1, 1, 1]), dtype=np.float32)\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.sim = ry.Simulation(self.C, ry.SimulationEngine.physx, verbose=4)\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.sim\n",
    "        del self.C\n",
    "    \n",
    "    # different step functions implemntation, once with direct actions and one with a komo solver endeffector\n",
    "        \n",
    "    # def step(self, action):\n",
    "    #     self.sim.step(action, self.tau, ry.ControlMode.velocity)\n",
    "    #     self.time += self.tau\n",
    "        \n",
    "    #     observation_joint = self.C.getJointState()\n",
    "    #     observation_box = self.C.getFrame('box').getPosition()\n",
    "    #     observation = np.concatenate((observation_joint, observation_box), axis=0)\n",
    "    #     reward = self.reward_fct(C)\n",
    "    #     truncated = (self.time >= self.time_limit) # terminated and truncated difference is super important\n",
    "    #     info = {\"no\": \"additional info\"}\n",
    "\n",
    "    #     return observation, reward, False, truncated, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        dx, dy, dtheta = action\n",
    "\n",
    "        ee = self.C.getFrame(\"plate\")\n",
    "        pos = ee.getPosition()\n",
    "        quat = ee.getQuaternion()\n",
    "\n",
    "        target_pos = pos + np.array([dx, dy, 0.0])\n",
    "        R_current = ry.Quaternion().set(quat)\n",
    "        R_delta = ry.Quaternion().setExp([0., 0., dtheta])\n",
    "        target_quat = (R_delta * R_current).asArr()\n",
    "\n",
    "        komo = ry.KOMO()\n",
    "        komo.setConfig(self.C, True)\n",
    "        komo.setTiming(1, 1, 1., 2)\n",
    "        komo.addObjective([], ry.FS.position, ['plate'], ry.OT.eq, [1e1], target_pos)\n",
    "        komo.addObjective([], ry.FS.quaternion, ['plate'], ry.OT.eq, [1e1], target_quat)\n",
    "        ret = ry.NLP_Solver(komo.nlp(), verbose=4).solve()\n",
    "\n",
    "        q_target = komo.getPath()[0]\n",
    "        self.sim.step(q_target - self.C.getJointState(), self.tau, ry.ControlMode.velocity)\n",
    "        self.time += self.tau\n",
    "\n",
    "        # self.C.view(False, 'simulating')\n",
    "          \n",
    "        observation_joint = self.C.getJointState() #more information\n",
    "        observation_box = self.C.getFrame('box').getPosition()\n",
    "        observation = np.concatenate((observation_joint, observation_box), axis=0)\n",
    "        reward = self.reward_fct(C)\n",
    "        terminated = False\n",
    "        truncated = (self.time >= self.time_limit) # terminated and truncated difference is super important\n",
    "        info = {\"no\": \"additional info\"}\n",
    "\n",
    "        # mistake in former repository\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.time = 0.\n",
    "        self.sim.setState(X0, q0)\n",
    "        self.sim.resetSplineRef()\n",
    "\n",
    "        observation_joint = self.C.getJointState()\n",
    "        observation_box = self.C.getFrame('box').getPosition()\n",
    "        observation = np.concatenate((observation_joint, observation_box), axis=0)\n",
    "        info = {\"no\": \"additional info\"}\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.C.view(False)\n",
    "\n",
    "        return observation, info\n",
    "        \n",
    "    def render(self):\n",
    "        self.C.view(False, f'RaiGym time {self.time} / {self.time_limit}')\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self.C.view_getRgb()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa708a",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "\n",
    "Great, the environment is currently set up. You defined the observation space, the action space and already defined the step function. One essential ingredient is missing: The Reward Function. You should pose the question: How do I make sure that the goal of pushing the 'box' with the 'plate' to the 'target' position?\n",
    "\n",
    "Tipp: C.eval(ry.FS.{members}, [{frames}]) will help you defining the reward. Familiarize yourself with the different members of the class FS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(C):\n",
    "    touch, _ = C.eval(ry.FS.negDistance, [\"plate\", \"box\"])\n",
    "    dist, _ = C.eval(ry.FS.positionDiff, [\"box\", \"target\"])\n",
    "    r = touch[0] - np.linalg.norm(dist)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f2904-b563-440a-8be8-20226544afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RaiGym(C, 10., reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe485581",
   "metadata": {},
   "source": [
    "### Test Functionality of the components you implemented and the physx simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99cf13a-44d5-4c5b-8e75-f0b5e3868f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "v = np.zeros(3)\n",
    "v[0] = 0.2 # 3 joints, linear velocity - + mit rechte daumen regel\n",
    "# v = np.zeros(env.q0.size)\n",
    "# v[0] = -1. # 7 joints, radial velocity - + mit rechte daumen regel\n",
    "print(v)\n",
    "\n",
    "t = 0\n",
    "while True:\n",
    "    t += 1\n",
    "    observation, reward, terminated, truncated, info = env.step(v)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        break\n",
    "    # print(\"reward: \", reward)\n",
    "    # if not (t%5):\n",
    "    #     env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ca991",
   "metadata": {},
   "source": [
    "### Now the actual policy learning happens\n",
    "\n",
    "We import SAC and PPO from the stable_baselines3 library, define checkpoints and a tensorboard logger. Learning the policy takes some time on a normal CPU, consider a total_timesteps of 200_000 or even more. Observe the training dynamics in the tensorbaord logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee831fc-436f-461f-9644-4ad6a6516094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc305e-5c9f-4798-b4e8-be5f890c54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./checkpoints_endeffectorActions/', name_prefix='rl_model')\n",
    "model = SAC(\"MlpPolicy\", env, gamma=0.99, learning_rate=1e-4, verbose=1, tensorboard_log=\"./tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd665ada-a480-4e9a-a3c2-aab5dba20761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=500_000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9650bc6",
   "metadata": {},
   "source": [
    "### Rollout of the trained policy\n",
    "\n",
    "Once the policy is trained, you can load it from a certain checkpoint\n",
    "\n",
    "### Task 4:\n",
    "\n",
    "Now you want to actually play the policy, therefore you need to implement a simple loop. Make sure that you observe the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load('./checkpoints_endeffectorActions/rl_model_199498_steps.zip', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06410a-0941-48f2-851c-3d6143e3060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "for t in range(200):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action) # super relevant Achim the legend\n",
    "    if not (t%10):\n",
    "        env.render()\n",
    "        print(\"reward: \", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfd32b",
   "metadata": {},
   "source": [
    "### (Bonus) Task 5:\n",
    "\n",
    "You did a great job! The last task considers again the step function of the environment. Beforehand the step function considered directly the joint space positions actions which were directly fed into the physx simulator. \n",
    "\n",
    "But what about reducing the action space to for example the x, y positions and theta orientation of the endeffector ('plate')? Define a new step function in the environment. \n",
    "\n",
    "Tipp: You will need to solve a komo problem in the step function to move the endeffector. Consider adjustments in the actionspace and in the functionality test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd5153-e8a3-4612-b314-914cf19cb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del g\n",
    "# del C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
