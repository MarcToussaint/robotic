{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e193f01d",
   "metadata": {},
   "source": [
    "# Intro: BotOp (Robot Operation) interface\n",
    "\n",
    "BotOp (=robot operation) defines a very narrow interface to control a real or simulated robot. While in initial years we tried all kinds of other interfaces (ROS-given, operational space control interfaces, controller state machines, etc), this one seems most pragmatic, simple, transparent, and compatible to our research work at the LIS research team. There is no ROS or complex IPC involved, just a few threads (communicating with hardwares) interfaced via BotOp.\n",
    "\n",
    "The interface essentially provides move methods to set or smoothly overwrite a spline reference for the robot. (Also compliance around the reference can be set.) Gripper methods to operate grippers. And getImage.. methods grab images or point clouds from the camera. That's basically it.\n",
    "\n",
    "There might be confusion about whether `BotOp` and `Simulation` are the same or similar or what's different. While they both are \"robot simulation interfaces\", they actually play very different roles and that's important to understand: `Simulation` is a direct interface to physics engines (by default Nvidia Physx), similar to a gym environment, with a *non-threaded* step function that you call explicitly. You typically use `Simulation` to train RL, evaluate controllers, or generate data offline, much faster than real time. In constrast, `BotOp` interfaces or emulates a real robot, running in real time. The interface methods are not a step function, but the exact same as for controlling the real robot: setter methods for control (setting reference splines, impedances, gripper states) and getter methods to get state information (getImage, get_q, etc). Your code runs in parallel to the real world (or the `BotOp` emulation) and needs to explicitly sync or get the state information. But yes, under the hood `BotOp` uses a `Simulation` as the underlying engine when in simulation mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e6960",
   "metadata": {},
   "source": [
    "## Sending motion based on IK\n",
    "\n",
    "We'll show how to make the robot move to pre-computed joint space poses, e.g. computed via IK. Later we modify this to *overwriting* the motion reference with high frequency, which essentially realizes MPC-style control.\n",
    "\n",
    "The first step (also for operating the real robot) is always to load a configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a434d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotic as ry\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "C = ry.Config()\n",
    "C.addFile(ry.raiPath('../rai-robotModels/scenarios/pandaSingle.g'))\n",
    "C.view(False, 'this is your workspace data structure C -- NOT THE SIMULTATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333c4b1",
   "metadata": {},
   "source": [
    "Next we will open a robot interface in simulation (`useRealRobot=False`). `True` would directly open communication to one or two pandas (depending no how many are defined in C). The `botsim/verbose` leads to the explicit verbosity when creating the simulator interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ry.params_add({'botsim/verbose': 2})\n",
    "bot = ry.BotOp(C, useRealRobot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad9bb7",
   "metadata": {},
   "source": [
    "Note the simulation window, showing that the simulation is running in a thread and the given *control reference time*.\n",
    "\n",
    "We define 2 reference poses, q0=home and q1=(2nd joint bend), so that we can move back and forth between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe800f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qHome = bot.get_qHome()\n",
    "q0 = qHome.copy()\n",
    "q1 = q0.copy()\n",
    "q1[1] = q1[1] + .2\n",
    "print(q0, q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f72e9b",
   "metadata": {},
   "source": [
    "The `moveTo` is the simplest way to move the robot from current to target. It internally creates a cubic B-spline to the target with optimal timing and follows it. The call is *non-blocking*. Also, your workspace config C is not kept in sync with the real/sim. If you want to wait till the motion is finished, you need to manually keep checking the `getTimeToEnd` (=time til the end of the given spline reference), and meanwhile staying sync'ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443856f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.moveTo(q1)\n",
    "\n",
    "while bot.getTimeToEnd()>0:\n",
    "    bot.sync(C, .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41eca8",
   "metadata": {},
   "source": [
    "The internal spline reference can be appended: As `moveTo` is non-blocking, you can append several moves like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('timeToEnd:', bot.getTimeToEnd())\n",
    "bot.moveTo(q0)\n",
    "print('timeToEnd:', bot.getTimeToEnd())\n",
    "bot.moveTo(q1)\n",
    "print('timeToEnd:', bot.getTimeToEnd())\n",
    "bot.moveTo(q0)\n",
    "\n",
    "while bot.getTimeToEnd()>0:\n",
    "    bot.sync(C, .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470f5db-251f-4917-9fbb-67d7107e70de",
   "metadata": {},
   "source": [
    "## Sending paths from KOMO\n",
    "\n",
    "The above shows moving towards a single target, where BopTo decides on the timing (see timeCost parameter of `moveTo`). But we can also send a spline computed, e.g., with path optimization. The example is taken from the [KOMO tutorial](komo_1_intro.html): we transition smoothly through 4 waypoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c6a53-f382-4d7f-879b-71f61428766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C.addFrame('way1'). setShape(ry.ST.marker, [.1]) .setPosition([.4, .2, 1.])\n",
    "C.addFrame('way2'). setShape(ry.ST.marker, [.1]) .setPosition([.4, .2, 1.4])\n",
    "C.addFrame('way3'). setShape(ry.ST.marker, [.1]) .setPosition([-.4, .2, 1.])\n",
    "C.addFrame('way4'). setShape(ry.ST.marker, [.1]) .setPosition([-.4, .2, 1.4])\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b845a-60ab-450c-a097-c5cc20f8ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "komo = ry.KOMO(C, 4, 10, 2, False)\n",
    "komo.addControlObjective([], 0, 1e-1)\n",
    "komo.addControlObjective([], 2, 1e0)\n",
    "komo.addObjective([1], ry.FS.positionDiff, ['l_gripper', 'way1'], ry.OT.eq, [1e1])\n",
    "komo.addObjective([2], ry.FS.positionDiff, ['l_gripper', 'way2'], ry.OT.eq, [1e1])\n",
    "komo.addObjective([3], ry.FS.positionDiff, ['l_gripper', 'way3'], ry.OT.eq, [1e1])\n",
    "komo.addObjective([4], ry.FS.positionDiff, ['l_gripper', 'way4'], ry.OT.eq, [1e1])\n",
    "komo.addObjective([4], ry.FS.jointState, [], ry.OT.eq, [1e1], [], order=1)\n",
    "\n",
    "ret = ry.NLP_Solver(komo.nlp(), verbose=0 ) .solve()\n",
    "print('solver return:', ret)\n",
    "path = komo.getPath()\n",
    "print('size of path:', path.shape)\n",
    "komo.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1734a-8761-430f-a3d0-c255aae76cac",
   "metadata": {},
   "source": [
    "We send this motion with the `move` method, explicitly specifying the timing to be 4 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444e4f7-2b9e-49a0-ae61-6f1f781f6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.move(path, [4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f404d45-43e0-4159-9fc0-6c36a73c9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "del komo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc71ed9",
   "metadata": {},
   "source": [
    "## Reactive control: Overwriting the reference (typical way to run medium frequency RL policies)\n",
    "\n",
    "Note: RL policies output new commands in every time step, let's say in 10Hz or so). The following is a default way to run such policies. The example below corresponds to when the policy outputs endeffector space targets. Then the policy outputs joint space targets, the IK can be skipped; then the policy outputs endeffector space deltas, just add them up to become targets. The IK method below is simple -- for more robust RL execution it should also check collisions and limits.\n",
    "\n",
    "BotOp is based on setting a spline reference. This becomes reactive, when we can smoothly overwrite the spline reference at any time. Technically (internally), smoothly overwriting means to take the current dynamic state (pose, velocity) and create a new cubic B-spline with current state as start and given target as end, with optimal timing. (See my B-Spline lecture notes.)\n",
    "\n",
    "To demonstrate this let's consider a more involved scenario, where the target is a frame that is randomly moving, and we use repeated IK in each cycle to track it. Let's first setup the scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this reference frame only appears in your workspace C - not the simulation!\n",
    "target = C.addFrame('target', 'table')\n",
    "target.setShape(ry.ST.marker, [.1])\n",
    "target.setRelativePosition([0., .3, .3])\n",
    "pos = target.getPosition()\n",
    "cen = pos.copy()\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8fb304",
   "metadata": {},
   "source": [
    "The following defines a very basic Inverse Kinematics method -- check the [KOMO tutorial](komo_1_intro.html) for more elaborate formulations. You can use a poseDiff instead of positionDiff, if you want full 6DOF endeffector control. Robust systems should also solve for collisions and limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a168d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IK(C, pos):\n",
    "    q0 = C.getJointState()\n",
    "    komo = ry.KOMO(C, 1, 1, 0, False) #one phase one time slice problem, with 'delta_t=1', order=0\n",
    "    komo.addObjective([], ry.FS.jointState, [], ry.OT.sos, [1e-1], q0) #cost: close to 'current state'\n",
    "    komo.addObjective([], ry.FS.jointState, [], ry.OT.sos, [1e-1], qHome) #cost: close to qHome\n",
    "    komo.addObjective([], ry.FS.positionDiff, ['l_gripper', 'target'], ry.OT.eq, [1e1]) #constraint: gripper position\n",
    "    \n",
    "    ret = ry.NLP_Solver(komo.nlp(), verbose=0) .solve()\n",
    "    \n",
    "    return [komo.getPath()[0], ret]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e13a25",
   "metadata": {},
   "source": [
    "The following does *not* move the robot: We use illustrate random IK solutions using the workspace C. No motion is sent to the real/simulated robot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(20):\n",
    "    time.sleep(.1)\n",
    "    pos = cen + .98 * (pos-cen) + 0.02 * np.random.randn(3)\n",
    "    target.setPosition(pos)\n",
    "    \n",
    "    q_target, ret = IK(C, pos)\n",
    "    print(ret)\n",
    "    C.setJointState(q_target)\n",
    "    C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d79cae",
   "metadata": {},
   "source": [
    "We now generate reative motion by smoothly overwriting the spline reference. Increasing time cost makes it more agressive (penalized total duration of estimated cubic spline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    bot.sync(C, .1) #keep the workspace C sync'ed to real/sim, and idle .1 sec\n",
    "    pos = cen + .98 * (pos-cen) + 0.02 * np.random.randn(3)\n",
    "    target.setPosition(pos)\n",
    "    \n",
    "    q_target, ret = IK(C, pos)\n",
    "    if ret.feasible:\n",
    "        bot.moveTo(q_target, timeCost=5., overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e22aa1",
   "metadata": {},
   "source": [
    "## Aborting motion\n",
    "\n",
    "Good practise is to always allow a user aborting motion execution. In this example, key 'q' will break the loop and call a home() (which is the same as moveTo(qHome, 1., True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09468d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(5):\n",
    "    print(t)\n",
    "    bot.moveTo(q1)\n",
    "    bot.wait(C) #same as 'loop sync til keypressed or endOfTime', but also raises user window\n",
    "    if bot.getKeyPressed()==ord('q'):\n",
    "        print(\"cancelled\")\n",
    "        break;\n",
    "        \n",
    "    bot.moveTo(q0)\n",
    "    bot.wait(C)\n",
    "    if bot.getKeyPressed()==ord('q'):\n",
    "        print(\"cancelled\")\n",
    "        break;\n",
    "\n",
    "bot.home(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867008d",
   "metadata": {},
   "source": [
    "## Gripper operation\n",
    "\n",
    "Gripper movements also do not block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.gripperMove(ry._left, width=.01, speed=.05)\n",
    "\n",
    "while not bot.gripperDone(ry._left):\n",
    "    bot.sync(C, .1)\n",
    "\n",
    "bot.gripperMove(ry._left, width=.075, speed=.1)\n",
    "\n",
    "while not bot.gripperDone(ry._left):\n",
    "    bot.sync(C, .1)\n",
    "\n",
    "bot.sync(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ec6ff",
   "metadata": {},
   "source": [
    "## Camera & Point Could\n",
    "\n",
    "BotOp also interfaces basic grabbing of image and depth. In simulation model, the sensor name needs to be a frame name that has camera attributes defined. On the real robot, a realsense camera is directly grabbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46440f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb, depth, points = bot.getImageDepthPcl('cameraWrist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd73706-2f9b-4c4d-93bd-cdba581408b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "axs = fig.subplots(1, 2)\n",
    "axs[0].imshow(rgb)\n",
    "axs[1].matshow(depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20815b9d",
   "metadata": {},
   "source": [
    "The returned `points` are a point could, that was directly computed from the depth image and the camera intrinsics. The intrinsics are given by the focal lengths (f_x, f_y) and image center (c_x, c_y). We can also manually compute the point cloud as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426eb3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fxycxy = bot.getCameraFxycxy(\"cameraWrist\")\n",
    "points2 = ry.depthImage2PointCloud(depth, fxycxy)\n",
    "np.linalg.norm(points - points2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11859a65",
   "metadata": {},
   "source": [
    "The point cloud is given relative to the camera frame. We can display it by creating a dedicates frame, attached to the camera frame, and setting its (purely visual) shape to be the point cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d2d8a-07ae-4dc3-afa5-3c90ac7d2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclFrame = C.addFrame('pcl', 'cameraWrist')\n",
    "pclFrame.setPointCloud(points, rgb)\n",
    "pclFrame.setColor([1.,0.,0.]) #only to see it when overlaying with truth\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b6ada-0c02-4566-ab42-9a1aa0054e56",
   "metadata": {},
   "source": [
    "## Grasp Test\n",
    "\n",
    "This is a minimalistic demo for box grasm\n",
    "It hard-codes the grasp using waypoints (-> should be replaced by model-based grasp planning (see komo-3-manipulation), or pcl-based grasp prediction)\n",
    "It uses the PhysX engine behind botop to actually simulate the grasp, using PD gains in the fingers to excert foces\n",
    "The focus of this test is how PhysX responds to setting PD and friction parameters of the grasp\n",
    "To this end, the lift is pretty fast... we want to potentially force a slip\n",
    "\n",
    "[literally translated from c++ test/21-grasp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca36e65-a8f7-4698-96d3-909475fe5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotic as ry\n",
    "import numpy as np\n",
    "\n",
    "# these are global parameters by which you can influence the friction, grasp force, etc...\n",
    "ry.params_add({\n",
    "    'physx/angularDamping': 0.1,\n",
    "    'physx/defaultFriction': 1.,  #reduce -> slip\n",
    "    'physx/defaultRestitution': .7, #quit bouncy\n",
    "    'botsim/verbose': 0})\n",
    "\n",
    "C = ry.Config()\n",
    "C.addFile(ry.raiPath(\"../rai-robotModels/scenarios/pandaSingle.g\"))\n",
    "\n",
    "C.addFrame(\"obj\") \\\n",
    "      .setPosition([-.25,.1,.7]) \\\n",
    "      .setShape(ry.ST.ssBox, [.04,.2,.1,.005]) \\\n",
    "      .setColor([1,.5,0]) \\\n",
    "      .setMass(.1) \\\n",
    "      .setContact(True)\n",
    "\n",
    "way0 = C.addFrame(\"way0\", \"obj\") .setShape(ry.ST.marker, [.1]) .setRelativePosition([0, 0, .2])\n",
    "way1 = C.addFrame(\"way1\", \"obj\") .setShape(ry.ST.marker, [.1]) .setRelativePosition([0, .0, .03])\n",
    "\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82a5c2-89ae-4f70-99ca-fe218f99f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 2 joint space waypoints from these endeff pose waypoints using komo\n",
    "komo = ry.KOMO()\n",
    "komo.setConfig(C, True)\n",
    "komo.setTiming(2., 1, 5., 0)\n",
    "komo.addControlObjective([], 0, 1e-0)\n",
    "komo.addObjective([], ry.FS.accumulatedCollisions, [], ry.OT.eq)\n",
    "komo.addObjective([], ry.FS.jointLimits, [], ry.OT.ineq)\n",
    "komo.addObjective([1.], ry.FS.poseDiff, [\"l_gripper\", \"way0\"], ry.OT.eq, [1e1])\n",
    "komo.addObjective([2.], ry.FS.poseDiff, [\"l_gripper\", \"way1\"], ry.OT.eq, [1e1])\n",
    "\n",
    "ret = ry.NLP_Solver() \\\n",
    "    .setProblem(komo.nlp()) \\\n",
    "    .setOptions(stopTolerance=1e-2, verbose=4 ) \\\n",
    "    .solve()\n",
    "print(ret)\n",
    "komo.set_viewer(C.get_viewer())\n",
    "komo.view(False, 'these are the joint space waypoints,\\n which are used as control points of the BotOp spline execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715f220-3b90-4025-8992-c79f62baefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ways = komo.getPath()\n",
    "\n",
    "C.getFrame('obj') .setPosition([-.25,.1,.7]) .setQuaternion([1,0,0,0])\n",
    "\n",
    "back_ways = np.concatenate([ways[0], C.getJointState()]) .reshape([2, C.getJointDimension()])\n",
    "print(back_ways)\n",
    "\n",
    "bot = ry.BotOp(C, useRealRobot=False)\n",
    "# bot.home(C)\n",
    "\n",
    "# open gripper\n",
    "bot.gripperMove(ry.ArgWord._left, .075, .1)\n",
    "bot.wait(C, forKeyPressed=False, forTimeToEnd=False, forGripper=True)\n",
    "\n",
    "# send a spline for execution, and wait til it's done\n",
    "bot.move(ways, [2., 3.])\n",
    "bot.wait(C, forKeyPressed=False, forTimeToEnd=True)\n",
    "\n",
    "# close gripper\n",
    "bot.gripperMove(ry.ArgWord._left, .03, .05)\n",
    "bot.wait(C, forKeyPressed=False, forTimeToEnd=False, forGripper=True)\n",
    "\n",
    "# send a spline for execution, and wait til it's done\n",
    "bot.move(back_ways, [.1, .5]) #very fast upward motion!\n",
    "bot.wait(C, forKeyPressed=False, forTimeToEnd=True)\n",
    "\n",
    "# open gripper\n",
    "bot.gripperMove(ry.ArgWord._left, .075, .1)\n",
    "bot.wait(C, forKeyPressed=False, forTimeToEnd=False, forGripper=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfbfcea",
   "metadata": {},
   "source": [
    "## Shutdown\n",
    "\n",
    "You always need to shut down processes (e.g. communication with the real robot) properly. That's done here by explicitly destroying the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "del bot\n",
    "del C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37869b",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "BotOp (and other parts of the rai code) use all kinds of internal parameters that can be configured. The best way to look which parameters actually are used/relevant is to retrospect print the list of parameters have been queried by the code so far. That gives an idea of which global parameters exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4031b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ry.params_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cb087",
   "metadata": {},
   "source": [
    "That might tell you a lot about what happend internally.\n",
    "\n",
    "In the context of BotOp, the parameter `botsim/engine` can also be set to `kin`, which would create a simulation without physics where merely the robot moves (and grasped object can be attached/detached). The `botsim/verbose` above leads to the explicit verbosity when creating the simulator interface.\n",
    "\n",
    "Parameters can be set in a local file `rai.cfg`, or manually in python  with the following calls -- but that need's to be done BEFORE BotOp is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c417c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ry.params_add({'botsim/verbose': 2., 'physx/motorKp': 10000., 'physx/motorKd': 1000.})\n",
    "ry.params_add({'botsim/engine': 'physx'}) #makes a big difference!\n",
    "ry.params_add({'physx/multibody': True}) #makes a big difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172dbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
