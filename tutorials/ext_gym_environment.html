

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Extension - Gym Environment Interface: minimal example &mdash; Robotic Python Library 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=01f34227"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture Script" href="../script/script.html" />
    <link rel="prev" title="Extension - NLP interface: Low-level NLP formulation and solving" href="ext_nlp_solvers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Robotic Python Library
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="config_1_intro.html">Intro: Configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="botop_1_intro.html">Intro: BotOp (Robot Operation) interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="botop_1_intro.html#Grasp-Test">Grasp Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="komo_1_intro.html">Intro: KOMO - Motion Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="botop_2_real_robot.html">BotOp-2: Real robot operation checklist &amp; first steps</a></li>
<li class="toctree-l2"><a class="reference internal" href="config_2_features.html">Config-2: Computing differentiable features &amp; collision evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="config_3_import_edit.html">Config-3: Importing, editing &amp; manipulating them</a></li>
<li class="toctree-l2"><a class="reference internal" href="komo_2_reporting.html">KOMO-2: Reporting &amp; explaining convergence</a></li>
<li class="toctree-l2"><a class="reference internal" href="komo_3_manipulation.html">KOMO-3: Manipulation Modelling &amp; Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="lgp_1_intro.html">LGP-1: First Mini Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="ext_physx_simulation.html">Extension - Simulation: Low-level stepping interface &amp; gym environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="ext_rendering.html">Extension - Rendering: Basic opengl, offscreen (headless), and interface to physics-based rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="ext_rrt.html">Extension - RRT: basic finding example</a></li>
<li class="toctree-l2"><a class="reference internal" href="ext_nlp_solvers.html">Extension - NLP interface: Low-level NLP formulation and solving</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Extension - Gym Environment Interface: minimal example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Task-1:">Task 1:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Task-3:">Task 3:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Test-Functionality-of-the-components-you-implemented-and-the-physx-simulator">Test Functionality of the components you implemented and the physx simulator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Now-the-actual-policy-learning-happens">Now the actual policy learning happens</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Rollout-of-the-trained-policy">Rollout of the trained policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Task-4:">Task 4:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#(Bonus)-Task-5:">(Bonus) Task 5:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../script/script.html">Lecture Script</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">robotic python API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Robotic Python Library</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../tutorials.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Extension - Gym Environment Interface: minimal example</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/ext_gym_environment.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Extension---Gym-Environment-Interface:-minimal-example">
<h1>Extension - Gym Environment Interface: minimal example<a class="headerlink" href="#Extension---Gym-Environment-Interface:-minimal-example" title="Link to this heading"></a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import robotic as ry
import gymnasium as gym
import numpy as np
print(&#39;ry version:&#39;, ry.__version__, ry.compiled())
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>C = ry.Config()
C.addFile(ry.raiPath(&#39;scenarios/pandaSingle.g&#39;))
C.view(False)

C.addFrame(&#39;box&#39;) \
    .setShape(ry.ST.ssBox, size=[.1,.1,.1,.005]) .setColor([1,.5,0]) \
    .setPosition([.06,.35,.7]) \
    .setMass(.1) \
    .setAttribute(&#39;friction&#39;, 0.01) # friction definition # initial noise with falling box?

C.addFrame(&#39;plate&#39;, &#39;l_gripper&#39;) \
    .setShape(ry.ST.ssBox, size=[.01,.2,.36,.0025]) .setColor([.5,1,0]) \
    .setRelativePosition([0,0,-.16])

C.addFrame(&#39;target&#39;) \
    .setShape(ry.ST.marker, size=[.1]) .setColor([0,1,0]) \
    .setPosition([.4,.3,.7]) \

#the following would make it a LOT simpler
C.setJointState([.0], [&#39;l_panda_joint2&#39;])
C.setJointState([.7], [&#39;l_panda_joint7&#39;])

C.setJointState([.007], [&#39;l_panda_finger_joint1&#39;]) #only cosmetics

q0 = C.getJointState()
X0 = C.getFrameState()

C.view()
</pre></div>
</div>
</div>
<section id="Task-1:">
<h2>Task 1:<a class="headerlink" href="#Task-1:" title="Link to this heading"></a></h2>
<p>The following code consists of a minimal Implementation of an environment in which the robot can operate. Currently the policy receives information about the internal state but no further information about the environment state. This is called open-loop Reinforcement Learning.</p>
<p>What do you need to change to make it a closed-loop Reinforcement Learning setting? Apply the changes to the current environment.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class RaiGym(gym.Env):
    metadata = {&quot;render_modes&quot;: [&quot;human&quot;, &quot;rgb_array&quot;], &quot;render_fps&quot;: 4}
    tau = .05
    time = 0.

    def __init__(self, C, time_limit, reward_fct, render_mode=None):
        self.C : ry.Config = C
        self.time_limit = time_limit
        self.reward_fct = reward_fct
        self.render_mode = render_mode
        # self.limits = self.C.getJointLimits()
        self.limits = [-10., 10.]
        self.q0 = self.C.getJointState()
        self.X0 = self.C.getFrameState()

        # here they need to configure open loop or closed loop RL, with and without box position
        #self.observation_space = gym.spaces.box.Box(self.limits[0], self.limits[1], shape=(self.q0.size,), dtype=np.float32)
        self.observation_space = gym.spaces.box.Box(self.limits[0], self.limits[1], shape=(self.q0.size + 3,), dtype=np.float32)

        #self.action_space = gym.spaces.box.Box(low=-1., high=1., shape=(self.q0.size,), dtype=np.float32)
        self.action_space = gym.spaces.Box(low=np.array([-1, -1, -1]), high=np.array([1, 1, 1]), dtype=np.float32)

        assert render_mode is None or render_mode in self.metadata[&quot;render_modes&quot;]
        self.render_mode = render_mode

        self.sim = ry.Simulation(self.C, ry.SimulationEngine.physx, verbose=4)

    def __del__(self):
        del self.sim
        del self.C

    # different step functions implemntation, once with direct actions and one with a komo solver endeffector

    # def step(self, action):
    #     self.sim.step(action, self.tau, ry.ControlMode.velocity)
    #     self.time += self.tau

    #     observation_joint = self.C.getJointState()
    #     observation_box = self.C.getFrame(&#39;box&#39;).getPosition()
    #     observation = np.concatenate((observation_joint, observation_box), axis=0)
    #     reward = self.reward_fct(C)
    #     truncated = (self.time &gt;= self.time_limit) # terminated and truncated difference is super important
    #     info = {&quot;no&quot;: &quot;additional info&quot;}

    #     return observation, reward, False, truncated, info

    def step(self, action):
        dx, dy, dtheta = action

        ee = self.C.getFrame(&quot;plate&quot;)
        pos = ee.getPosition()
        quat = ee.getQuaternion()

        target_pos = pos + np.array([dx, dy, 0.0])
        R_current = ry.Quaternion().set(quat)
        R_delta = ry.Quaternion().setExp([0., 0., dtheta])
        target_quat = (R_delta * R_current).asArr()

        komo = ry.KOMO()
        komo.setConfig(self.C, True)
        komo.setTiming(1, 1, 1., 2)
        komo.addObjective([], ry.FS.position, [&#39;plate&#39;], ry.OT.eq, [1e1], target_pos)
        komo.addObjective([], ry.FS.quaternion, [&#39;plate&#39;], ry.OT.eq, [1e1], target_quat)
        ret = ry.NLP_Solver(komo.nlp(), verbose=4).solve()

        q_target = komo.getPath()[0]
        self.sim.step(q_target - self.C.getJointState(), self.tau, ry.ControlMode.velocity)
        self.time += self.tau

        # self.C.view(False, &#39;simulating&#39;)

        observation_joint = self.C.getJointState() #more information
        observation_box = self.C.getFrame(&#39;box&#39;).getPosition()
        observation = np.concatenate((observation_joint, observation_box), axis=0)
        reward = self.reward_fct(C)
        terminated = False
        truncated = (self.time &gt;= self.time_limit) # terminated and truncated difference is super important
        info = {&quot;no&quot;: &quot;additional info&quot;}

        # mistake in former repository
        return observation, reward, terminated, truncated, info


    def reset(self, seed=None):
        super().reset(seed=seed)

        self.time = 0.
        self.sim.setState(X0, q0)
        self.sim.resetSplineRef()

        observation_joint = self.C.getJointState()
        observation_box = self.C.getFrame(&#39;box&#39;).getPosition()
        observation = np.concatenate((observation_joint, observation_box), axis=0)
        info = {&quot;no&quot;: &quot;additional info&quot;}

        if self.render_mode == &quot;human&quot;:
            self.C.view(False)

        return observation, info

    def render(self):
        self.C.view(False, f&#39;RaiGym time {self.time} / {self.time_limit}&#39;)
        if self.render_mode == &quot;rgb_array&quot;:
            return self.C.view_getRgb()
<br/></pre></div>
</div>
</div>
</section>
<section id="Task-3:">
<h2>Task 3:<a class="headerlink" href="#Task-3:" title="Link to this heading"></a></h2>
<p>Great, the environment is currently set up. You defined the observation space, the action space and already defined the step function. One essential ingredient is missing: The Reward Function. You should pose the question: How do I make sure that the goal of pushing the ‘box’ with the ‘plate’ to the ‘target’ position?</p>
<p>Tipp: C.eval(ry.FS.{members}, [{frames}]) will help you defining the reward. Familiarize yourself with the different members of the class FS.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def reward_function(C):
    touch, _ = C.eval(ry.FS.negDistance, [&quot;plate&quot;, &quot;box&quot;])
    dist, _ = C.eval(ry.FS.positionDiff, [&quot;box&quot;, &quot;target&quot;])
    r = touch[0] - np.linalg.norm(dist)
    return r
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env = RaiGym(C, 10., reward_function)
</pre></div>
</div>
</div>
</section>
<section id="Test-Functionality-of-the-components-you-implemented-and-the-physx-simulator">
<h2>Test Functionality of the components you implemented and the physx simulator<a class="headerlink" href="#Test-Functionality-of-the-components-you-implemented-and-the-physx-simulator" title="Link to this heading"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env.reset()

v = np.zeros(3)
v[0] = 0.2 # 3 joints, linear velocity - + mit rechte daumen regel
# v = np.zeros(env.q0.size)
# v[0] = -1. # 7 joints, radial velocity - + mit rechte daumen regel
print(v)

t = 0
while True:
    t += 1
    observation, reward, terminated, truncated, info = env.step(v)
    done = terminated or truncated
    if done:
        break
    # print(&quot;reward: &quot;, reward)
    # if not (t%5):
    #     env.render()
</pre></div>
</div>
</div>
</section>
<section id="Now-the-actual-policy-learning-happens">
<h2>Now the actual policy learning happens<a class="headerlink" href="#Now-the-actual-policy-learning-happens" title="Link to this heading"></a></h2>
<p>We import SAC and PPO from the stable_baselines3 library, define checkpoints and a tensorboard logger. Learning the policy takes some time on a normal CPU, consider a total_timesteps of 200_000 or even more. Observe the training dynamics in the tensorbaord logger.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from stable_baselines3 import SAC, PPO
from stable_baselines3.common.callbacks import CheckpointCallback
import torch
print(torch.__version__)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=&#39;./checkpoints_endeffectorActions/&#39;, name_prefix=&#39;rl_model&#39;)
model = SAC(&quot;MlpPolicy&quot;, env, gamma=0.99, learning_rate=1e-4, verbose=1, tensorboard_log=&quot;./tensorboard/&quot;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model.learn(total_timesteps=500_000, callback=checkpoint_callback)
</pre></div>
</div>
</div>
</section>
<section id="Rollout-of-the-trained-policy">
<h2>Rollout of the trained policy<a class="headerlink" href="#Rollout-of-the-trained-policy" title="Link to this heading"></a></h2>
<p>Once the policy is trained, you can load it from a certain checkpoint</p>
</section>
<section id="Task-4:">
<h2>Task 4:<a class="headerlink" href="#Task-4:" title="Link to this heading"></a></h2>
<p>Now you want to actually play the policy, therefore you need to implement a simple loop. Make sure that you observe the reward.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model = SAC.load(&#39;./checkpoints_endeffectorActions/rl_model_199498_steps.zip&#39;, env=env)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>obs, info = env.reset()
for t in range(200):
    action, _state = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action) # super relevant Achim the legend
    if not (t%10):
        env.render()
        print(&quot;reward: &quot;, reward)
</pre></div>
</div>
</div>
</section>
<section id="(Bonus)-Task-5:">
<h2>(Bonus) Task 5:<a class="headerlink" href="#(Bonus)-Task-5:" title="Link to this heading"></a></h2>
<p>You did a great job! The last task considers again the step function of the environment. Beforehand the step function considered directly the joint space positions actions which were directly fed into the physx simulator.</p>
<p>But what about reducing the action space to for example the x, y positions and theta orientation of the endeffector (‘plate’)? Define a new step function in the environment.</p>
<p>Tipp: You will need to solve a komo problem in the step function to move the endeffector. Consider adjustments in the actionspace and in the functionality test function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># del model
# del g
# del C
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ext_nlp_solvers.html" class="btn btn-neutral float-left" title="Extension - NLP interface: Low-level NLP formulation and solving" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../script/script.html" class="btn btn-neutral float-right" title="Lecture Script" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Marc Toussaint.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>